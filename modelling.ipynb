{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkRAPLctNM3h"
      },
      "source": [
        "# Content Based Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGnVd6CWPsLx",
        "outputId": "9d041d95-e9c1-4cd5-c2a5-cd415717842f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLmbjQV-PupC",
        "outputId": "7f484ed5-8c5e-4125-98c4-37520a21bbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/recipe\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tOTL0HGvNM3j"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCTvC_iTNM3l",
        "outputId": "e0384194-44e2-47ab-9b1c-a6555a201033"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(212255, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk0JKjd5oAqJ",
        "outputId": "753d7c04-9c67-4ee7-dfeb-048701c3cbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==4.3.0 (from -r requirements.txt (line 1))\n",
            "  Downloading gensim-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib==1.2.0 (from -r requirements.txt (line 2))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.8.1)\n",
            "Collecting numpy==1.24.3 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.5.3 (from -r requirements.txt (line 6))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.24.1 (from -r requirements.txt (line 7))\n",
            "  Downloading scikit-learn-0.24.1.tar.gz (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-surprise==1.1.1 (from -r requirements.txt (line 8))\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 9))\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit==1.25.0 (from -r requirements.txt (line 10))\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.0->-r requirements.txt (line 1)) (6.4.0)\n",
            "Collecting FuzzyTM>=0.4.0 (from gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (2024.4.28)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 6)) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.1->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise==1.1.1->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (5.3.3)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/lib/python3/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (4.6.4)\n",
            "Collecting packaging>=20.0 (from matplotlib==3.7.1->-r requirements.txt (line 3))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=6.2.0 (from matplotlib==3.7.1->-r requirements.txt (line 3))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (3.20.3)\n",
            "Collecting pyarrow>=6.0 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pympler<2,>=0.9 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.18 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (4.11.0)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.25.0->-r requirements.txt (line 10)) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (4.22.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (0.12.1)\n",
            "Collecting pyfume (from FuzzyTM>=0.4.0->gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit==1.25.0->-r requirements.txt (line 10)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit==1.25.0->-r requirements.txt (line 10)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit==1.25.0->-r requirements.txt (line 10)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit==1.25.0->-r requirements.txt (line 10)) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit==1.25.0->-r requirements.txt (line 10)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit==1.25.0->-r requirements.txt (line 10)) (2.17.2)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.25.0->-r requirements.txt (line 10))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.25.0->-r requirements.txt (line 10)) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.25.0->-r requirements.txt (line 10)) (0.1.2)\n",
            "INFO: pip is looking at multiple versions of pyfume to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyfume (from FuzzyTM>=0.4.0->gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
            "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit==1.25.0->-r requirements.txt (line 10)) (2024.1)\n",
            "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0->-r requirements.txt (line 1))\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scikit-learn, scikit-surprise, fst-pso, miniful\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for scikit-learn \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp310-cp310-linux_x86_64.whl size=1827805 sha256=b9f3327637c589a269c20c0c4428c07aaeecb2b43206951a90420c7a63592181\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/11/fb/6f834df4a72a79fcd80a6ab5e3098cee14dc66bda025915ace\n",
            "  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20431 sha256=cb1b92f0fb6898910015fa3db12f1f1ac68fb8055718a529a8163061bbfdb608\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/1b/42/88a19f6b3896c2230d5053832f208976cddf70625885201d06\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=280e8f36ee97832c4cb2d0f4d8089d03eff2bc19ef1ff25f88c76115ecf55918\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/86/8f/7bb7f6472e2c84de7addfc1a5cd7fd647f00d8fb640da9ea9a\n",
            "Successfully built scikit-surprise fst-pso miniful\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBcVPYQkoBru",
        "outputId": "597310ce-42d2-4175-c610-612e8c324b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbl37MGaoBvE",
        "outputId": "b7dd81f8-05db-4172-be99-539cac49c22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/recipe/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/recipe/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m recipe No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m recipe No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m recipe No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m recipe No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 3.464s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMHf-qOvoBzg",
        "outputId": "c02067d9-b797-46f3-b7dd-22aec7d6662b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.121.120.233\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.716s\n",
            "Usage: lt --port [num] <options>\n",
            "\n",
            "Options:\n",
            "  -p, --port                Internal HTTP server port                 [required]\n",
            "  -h, --host                Upstream server providing forwarding\n",
            "                                             [default: \"https://localtunnel.me\"]\n",
            "  -s, --subdomain           Request this subdomain\n",
            "  -l, --local-host          Tunnel traffic to this host instead of localhost,\n",
            "                            override Host header to this host\n",
            "      --local-https         Tunnel traffic to a local HTTPS server     [boolean]\n",
            "      --local-cert          Path to certificate PEM file for local HTTPS server\n",
            "      --local-key           Path to certificate key file for local HTTPS server\n",
            "      --local-ca            Path to certificate authority file for self-signed\n",
            "                            certificates\n",
            "      --allow-invalid-cert  Disable certificate checks for your local HTTPS\n",
            "                            server (ignore cert/key/ca options)        [boolean]\n",
            "  -o, --open                Opens the tunnel URL in your browser\n",
            "      --print-requests      Print basic request info                   [boolean]\n",
            "      --help                Show this help and exit                    [boolean]\n",
            "      --version             Show version number                        [boolean]\n",
            "\n",
            "Missing required argument: port\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app_reciperecs.py &>/content/logs.txt & npx localtunnel port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StYkViSpoB3z",
        "outputId": "28797061-9b58-4559-d323-3b7bb5078fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.449s\n",
            "your url is: https://fruity-parts-suffer.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tiu41Di35D6B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-ihHdhqjoB-r",
        "outputId": "d00f499c-afec-418e-c889-877c4f4cef7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://4xkxcd9rgrc-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN067wZr3gAn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ8QFVQc3gDW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8BGGVqL3gGZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "data =pd.read_pickle(\"food.pkl\")\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "iBivS-bBNM3n",
        "outputId": "b2f5ce35-b2bb-4e97-8309-ea38393292fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c4fee2b6-8ed9-4936-b0be-70725ef42e20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>id</th>\n",
              "      <th>minutes</th>\n",
              "      <th>contributor_id</th>\n",
              "      <th>submitted</th>\n",
              "      <th>tags</th>\n",
              "      <th>n_steps</th>\n",
              "      <th>steps</th>\n",
              "      <th>description</th>\n",
              "      <th>ingredients</th>\n",
              "      <th>...</th>\n",
              "      <th>submitted_month</th>\n",
              "      <th>submitted_year</th>\n",
              "      <th>dairy-free</th>\n",
              "      <th>gluten-free</th>\n",
              "      <th>low-carb</th>\n",
              "      <th>vegan</th>\n",
              "      <th>vegetarian</th>\n",
              "      <th>recipe_id</th>\n",
              "      <th>average_rating</th>\n",
              "      <th>votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>168477</th>\n",
              "      <td>sesame dipping sauce</td>\n",
              "      <td>353233</td>\n",
              "      <td>10</td>\n",
              "      <td>406741</td>\n",
              "      <td>2009-01-31</td>\n",
              "      <td>['lactose', '15-minutes-or-less', 'time-to-mak...</td>\n",
              "      <td>2</td>\n",
              "      <td>['in a small bowl , whisk together soy sauce ,...</td>\n",
              "      <td>this works well for dipping pot stickers in as...</td>\n",
              "      <td>['soy sauce', 'rice vinegar', 'sesame oil', 'a...</td>\n",
              "      <td>...</td>\n",
              "      <td>Jan</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>353233</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201337</th>\n",
              "      <td>unattended rib roast</td>\n",
              "      <td>97973</td>\n",
              "      <td>112</td>\n",
              "      <td>152393</td>\n",
              "      <td>2004-08-17</td>\n",
              "      <td>['time-to-make', 'main-ingredient', 'preparati...</td>\n",
              "      <td>6</td>\n",
              "      <td>['at noon , preheat oven to 375 degrees', 'sea...</td>\n",
              "      <td>i found this recipe in the paper and thought i...</td>\n",
              "      <td>['standing rib roast', 'salt and pepper']</td>\n",
              "      <td>...</td>\n",
              "      <td>Aug</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>97973</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173114</th>\n",
              "      <td>slow cooker mashed potatoes</td>\n",
              "      <td>265017</td>\n",
              "      <td>135</td>\n",
              "      <td>290107</td>\n",
              "      <td>2007-11-12</td>\n",
              "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
              "      <td>6</td>\n",
              "      <td>['in a mixing bowl , combine cream cheese , so...</td>\n",
              "      <td>this recipe is from taste of home magazine. it...</td>\n",
              "      <td>['cream cheese', 'sour cream', 'butter', 'drie...</td>\n",
              "      <td>...</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>265017</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29931</th>\n",
              "      <td>butterscotch toffee cookies</td>\n",
              "      <td>288112</td>\n",
              "      <td>25</td>\n",
              "      <td>668077</td>\n",
              "      <td>2008-02-23</td>\n",
              "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
              "      <td>8</td>\n",
              "      <td>['mix shortening and both sugars with electric...</td>\n",
              "      <td>in preparation for a bake sale, i changed a re...</td>\n",
              "      <td>['shortening', 'sugar', 'brown sugar', 'egg', ...</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>288112</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9300</th>\n",
              "      <td>au gratin hash browns casserole</td>\n",
              "      <td>94740</td>\n",
              "      <td>70</td>\n",
              "      <td>126418</td>\n",
              "      <td>2004-06-30</td>\n",
              "      <td>['time-to-make', 'course', 'preparation', 'cas...</td>\n",
              "      <td>6</td>\n",
              "      <td>['oven@ 350', 'in a large bowl combine first 5...</td>\n",
              "      <td>i think i found this recipe in my \"goody bag\" ...</td>\n",
              "      <td>['cream of chicken soup', 'sour cream', 'marga...</td>\n",
              "      <td>...</td>\n",
              "      <td>Jun</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94740</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4fee2b6-8ed9-4936-b0be-70725ef42e20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4fee2b6-8ed9-4936-b0be-70725ef42e20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4fee2b6-8ed9-4936-b0be-70725ef42e20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2c2b0d86-e5a4-4f06-ab17-b9fa6faf6908\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c2b0d86-e5a4-4f06-ab17-b9fa6faf6908')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2c2b0d86-e5a4-4f06-ab17-b9fa6faf6908 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                   name      id  minutes contributor_id  \\\n",
              "168477             sesame dipping sauce  353233       10         406741   \n",
              "201337             unattended rib roast   97973      112         152393   \n",
              "173114      slow cooker mashed potatoes  265017      135         290107   \n",
              "29931       butterscotch toffee cookies  288112       25         668077   \n",
              "9300    au gratin hash browns casserole   94740       70         126418   \n",
              "\n",
              "        submitted                                               tags  n_steps  \\\n",
              "168477 2009-01-31  ['lactose', '15-minutes-or-less', 'time-to-mak...        2   \n",
              "201337 2004-08-17  ['time-to-make', 'main-ingredient', 'preparati...        6   \n",
              "173114 2007-11-12  ['time-to-make', 'course', 'main-ingredient', ...        6   \n",
              "29931  2008-02-23  ['30-minutes-or-less', 'time-to-make', 'course...        8   \n",
              "9300   2004-06-30  ['time-to-make', 'course', 'preparation', 'cas...        6   \n",
              "\n",
              "                                                    steps  \\\n",
              "168477  ['in a small bowl , whisk together soy sauce ,...   \n",
              "201337  ['at noon , preheat oven to 375 degrees', 'sea...   \n",
              "173114  ['in a mixing bowl , combine cream cheese , so...   \n",
              "29931   ['mix shortening and both sugars with electric...   \n",
              "9300    ['oven@ 350', 'in a large bowl combine first 5...   \n",
              "\n",
              "                                              description  \\\n",
              "168477  this works well for dipping pot stickers in as...   \n",
              "201337  i found this recipe in the paper and thought i...   \n",
              "173114  this recipe is from taste of home magazine. it...   \n",
              "29931   in preparation for a bake sale, i changed a re...   \n",
              "9300    i think i found this recipe in my \"goody bag\" ...   \n",
              "\n",
              "                                              ingredients  ...  \\\n",
              "168477  ['soy sauce', 'rice vinegar', 'sesame oil', 'a...  ...   \n",
              "201337          ['standing rib roast', 'salt and pepper']  ...   \n",
              "173114  ['cream cheese', 'sour cream', 'butter', 'drie...  ...   \n",
              "29931   ['shortening', 'sugar', 'brown sugar', 'egg', ...  ...   \n",
              "9300    ['cream of chicken soup', 'sour cream', 'marga...  ...   \n",
              "\n",
              "        submitted_month  submitted_year  dairy-free  gluten-free  low-carb  \\\n",
              "168477              Jan            2009           0            0         0   \n",
              "201337              Aug            2004           0            0         1   \n",
              "173114              Nov            2007           0            0         0   \n",
              "29931               Feb            2008           0            0         0   \n",
              "9300                Jun            2004           0            0         0   \n",
              "\n",
              "        vegan  vegetarian  recipe_id average_rating  votes  \n",
              "168477      1           1     353233            5.0      2  \n",
              "201337      0           0      97973            5.0      1  \n",
              "173114      0           0     265017            5.0      2  \n",
              "29931       0           0     288112            4.0      1  \n",
              "9300        0           0      94740            5.0      1  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Sampled Dataset: (53064, 28)\n"
          ]
        }
      ],
      "source": [
        "# Sample 25% of the dataset\n",
        "sampled_data = data.sample(frac=0.25, random_state=42)\n",
        "\n",
        "# View sample\n",
        "display(sampled_data.head())\n",
        "\n",
        "# Print the shape of the sampled dataset\n",
        "print(\"Shape of Sampled Dataset:\", sampled_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyL3rkFXNM3o",
        "outputId": "82d94aad-07eb-4979-e594-11ba3a7ba8b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbchuvaBNM3p"
      },
      "outputs": [],
      "source": [
        "# Create a function for tokenizer\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
        "\n",
        "def recipe_tokenizer(sentence):\n",
        "    # remove punctuation and set to lower case\n",
        "    for punctuation_mark in string.punctuation:\n",
        "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
        "\n",
        "    # split sentence into words\n",
        "    listofwords = sentence.split(' ')\n",
        "    listofstemmed_words = []\n",
        "\n",
        "    # remove stopwords and any tokens that are just empty strings\n",
        "    for word in listofwords:\n",
        "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
        "            # Stem words\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            listofstemmed_words.append(stemmed_word)\n",
        "\n",
        "    return listofstemmed_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuAR_3oPNM3p"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEw7tPoXNM3q"
      },
      "outputs": [],
      "source": [
        "# Function for word embedding using Word2Vec\n",
        "def word_embedding(sampled_data, column):\n",
        "    # Tokenize the text data\n",
        "    tokenized_data = sampled_data[column].apply(recipe_tokenizer)\n",
        "\n",
        "    # Train a Word2Vec model\n",
        "    model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Create word embeddings for each word in the vocabulary\n",
        "    embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9mWygwo-8oL"
      },
      "source": [
        "# **Cosine Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRb64TFgNM3r"
      },
      "outputs": [],
      "source": [
        "# Function to pre-compute and store the combined embeddings\n",
        "def precompute_embeddings(sampled_data):\n",
        "    # Step 1: Process 'ingredients' using word2vec and create word embeddings\n",
        "    embeddings = word_embedding(sampled_data, 'ingredients')\n",
        "\n",
        "    # Step 2: Concatenate relevant columns (excluding 'ingredients')\n",
        "    sampled_data['text_data'] = sampled_data[['name', 'tags', 'description']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    # Step 3: Preprocess the text data (example: lowercase conversion)\n",
        "    sampled_data['text_data'] = sampled_data['text_data'].str.lower()\n",
        "\n",
        "    # Step 4: Vectorize the text data (excluding 'ingredients') using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(min_df=5,\n",
        "                                 tokenizer=recipe_tokenizer)\n",
        "    vectorized_data = vectorizer.fit_transform(sampled_data['text_data'])\n",
        "\n",
        "    # Step 5: Retrieve the word embeddings for 'ingredients'\n",
        "    ingredient_embeddings = [np.mean([embeddings[word] for word in recipe_tokenizer(ingredients) if word in embeddings]\n",
        "                                      or [np.zeros(100)], axis=0) for ingredients in sampled_data['ingredients']]\n",
        "\n",
        "    # Step 6: Combine the vectorized data and ingredient embeddings\n",
        "    combined_embeddings = np.concatenate([vectorized_data.toarray(), np.array(ingredient_embeddings)], axis=1)\n",
        "\n",
        "    # Step 7: Store combined embeddings in pkl file\n",
        "    with open('combined_embeddings.pkl', 'wb') as f:\n",
        "        pickle.dump(combined_embeddings, f)\n",
        "\n",
        "    # Step 8: Store the trained TF-IDF vectorizer model in a separate pkl file\n",
        "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "        pickle.dump(vectorizer, f)\n",
        "\n",
        "    # Step 9: Done!\n",
        "    print(\"Text data and TF-IDF vectorizer model stored in pkl files!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsCfHnqRNM3s",
        "outputId": "44b5ebfe-a60b-4388-f0fc-d43d3acda9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text data and TF-IDF vectorizer model stored in pkl files!\n"
          ]
        }
      ],
      "source": [
        "# store vectorized data and the trained TF-IDF vectorizer model from sampled data\n",
        "precompute_embeddings(sampled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-7xTa0qNM3t"
      },
      "outputs": [],
      "source": [
        "# Function to load the combined embeddings and TF-IDF vectorizer model\n",
        "def load_embeddings_and_vectorizer():\n",
        "    with open('combined_embeddings.pkl', 'rb') as f:\n",
        "        combined_embeddings = pickle.load(f)\n",
        "    with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    return combined_embeddings, vectorizer\n",
        "\n",
        "# Function for finding recipes\n",
        "def find_similar_recipes_cosie(sampled_data, user_input, num_similar=5):\n",
        "    try:\n",
        "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
        "    except FileNotFoundError:\n",
        "        precompute_embeddings(sampled_data)\n",
        "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
        "\n",
        "    # Process user input\n",
        "    # Create a DataFrame for user input\n",
        "    user_data = pd.DataFrame({'text_data': [user_input]})\n",
        "    user_data['text_data'] = user_data['text_data'].str.lower()\n",
        "\n",
        "    # Vectorize the user input using the provided vectorizer\n",
        "    user_vectorized_data = vectorizer.transform(user_data['text_data'])\n",
        "\n",
        "    # Ensure the number of features in user_vectorized_data matches with combined_embeddings\n",
        "    num_missing_features = combined_embeddings.shape[1] - user_vectorized_data.shape[1]\n",
        "    if num_missing_features > 0:\n",
        "        # Add zero columns to user_vectorized_data to match the number of features\n",
        "        user_vectorized_data = np.pad(user_vectorized_data.toarray(), ((0, 0), (0, num_missing_features)))\n",
        "\n",
        "    # Compute cosine similarity with user input\n",
        "    cosine_sim_matrix = cosine_similarity(user_vectorized_data, combined_embeddings)\n",
        "\n",
        "    # Retrieve similar recipe indices\n",
        "    similar_recipes = cosine_sim_matrix[0].argsort()[::-1][:num_similar]\n",
        "\n",
        "    # Get similar recipe names from food_df\n",
        "    similar_recipe_names = sampled_data.iloc[similar_recipes]['name'].tolist()\n",
        "\n",
        "    return similar_recipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "0cptkXaTNM3u",
        "outputId": "7517a4a1-7896-4772-a699-76f1cb86573b"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "positional indexers are out-of-bounds",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3947\u001b[0m         \"\"\"\n\u001b[0;32m-> 3948\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3949\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3932\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3933\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-95cf106a5867>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfind_similar_recipes_cosie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"japanese dishes vegetarian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-fbaad5ba7598>\u001b[0m in \u001b[0;36mfind_similar_recipes_cosie\u001b[0;34m(sampled_data, user_input, num_similar)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Get similar recipe names from food_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msimilar_recipe_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimilar_recipes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilar_recipe_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "find_similar_recipes_cosie(sampled_data, \"japanese dishes vegetarian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaONXkSi-u1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_pXLEc-w9C"
      },
      "source": [
        "# K-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2ch4dmN_NXs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usKq0miV-u-w"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define tokenizer function\n",
        "def recipe_tokenizer(sentence):\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    ENGLISH_STOP_WORDS = stopwords.words('english')\n",
        "    for punctuation_mark in string.punctuation:\n",
        "        sentence = sentence.replace(punctuation_mark, '').lower()\n",
        "    listofwords = sentence.split(' ')\n",
        "    listofstemmed_words = []\n",
        "    for word in listofwords:\n",
        "        if (not word in ENGLISH_STOP_WORDS) and (word != ''):\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            listofstemmed_words.append(stemmed_word)\n",
        "    return listofstemmed_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze1kBMSk_J55",
        "outputId": "ae7ba4d4-ee0c-4d3e-ef85-dbb82cb6c601"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hcLTI0fL_Rjp",
        "outputId": "fcc8a925-6c20-4d6a-e0f5-7f048c64e1fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "NearestNeighbors(metric='cosine')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Preprocess text data\n",
        "sampled_data['text_data'] = sampled_data[['name', 'tags', 'description']].astype(str).agg(' '.join, axis=1)\n",
        "sampled_data['text_data'] = sampled_data['text_data'].str.lower()\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=5, tokenizer=recipe_tokenizer)\n",
        "X = vectorizer.fit_transform(sampled_data['text_data'])\n",
        "\n",
        "# Train k-NN model\n",
        "knn_model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "knn_model.fit(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuA88fiZ_XfK",
        "outputId": "18253e74-fdc2-4f5e-be73-cfcf6caf34dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['knn_model.joblib']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Save vectorizer and model\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
        "joblib.dump(knn_model, 'knn_model.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DXh7OP3_a0_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to find similar recipes\n",
        "def find_similar_recipes_knn(query, vectorizer_path='tfidf_vectorizer.joblib', model_path='knn_model.joblib', num_similar=5):\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "    model = joblib.load(model_path)\n",
        "    query_vectorized = vectorizer.transform([query.lower()])\n",
        "    distances, indices = model.kneighbors(query_vectorized)\n",
        "    similar_recipes = sampled_data.iloc[indices[0]]['name'].tolist()\n",
        "    return similar_recipes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lOa0mq_dSv",
        "outputId": "2d13ee91-a755-43b8-c993-317f5ca29c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar recipes to 'japanese dishes vegetarian' are:\n",
            "japanese cucumber salad   sunomono\n",
            "japanese sesame dipping sauce\n",
            "japanese noodle soup\n",
            "kabu    japanese turnip pickles\n",
            "vegetarian yakisoba\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test the recommendation system\n",
        "query = \"japanese dishes vegetarian\"\n",
        "similar_recipes = find_similar_recipes_knn(query)\n",
        "print(\"Similar recipes to '{}' are:\".format(query))\n",
        "for recipe in similar_recipes:\n",
        "    print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FnvHUt5_eu0"
      },
      "source": [
        "# **K-Means**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcJb6rMO_qoy"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier  # Already imported\n",
        "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT7bOYzI_tm8"
      },
      "outputs": [],
      "source": [
        "kmeans_model = KMeans(n_clusters=10, random_state=42)  # Adjust n_clusters as needed\n",
        "kmeans_model.fit(X)\n",
        "labels_kmeans = kmeans_model.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sgiyhus_zsU",
        "outputId": "397d52d7-7fe6-45e2-e3c4-5ccc54d3f188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar recipes to 'japanese dishes vegetarian' are:\n",
            "sesame dipping sauce\n",
            "unattended rib roast\n",
            "au gratin hash browns casserole\n"
          ]
        }
      ],
      "source": [
        "def find_similar_recipes_kmeans(query, kmeans_model, num_similar=3):\n",
        "  query_vectorized = vectorizer.transform([query.lower()])\n",
        "  query_cluster = kmeans_model.predict(query_vectorized)[0]\n",
        "  similar_recipes = sampled_data[sampled_data['name'].str.lower().apply(lambda x: kmeans_model.predict(vectorizer.transform([x]))[0]) == query_cluster]\n",
        "  return similar_recipes['name'].tolist()[:num_similar]\n",
        "\n",
        "# Test the recommendation system\n",
        "query = \"japanese dishes vegetarian\"\n",
        "similar_recipes = find_similar_recipes_kmeans(query, kmeans_model)\n",
        "print(\"Similar recipes to '{}' are:\".format(query))\n",
        "for recipe in similar_recipes:\n",
        "  print(recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVqEXvA_9z3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSuNyKOY_-Yx"
      },
      "source": [
        "# **DBSCAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "p6HpVjUQACYo",
        "outputId": "45e6d972-898a-4b14-af19-1612fe8f915e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DBSCAN(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DBSCAN</label><div class=\"sk-toggleable__content\"><pre>DBSCAN(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "DBSCAN(metric='cosine')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Train DBSCAN model\n",
        "dbscan_model = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n",
        "dbscan_model.fit(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pi7T9IDC-ib"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpvGg_rBAHVt"
      },
      "outputs": [],
      "source": [
        "def find_similar_recipes_DB(query, vectorizer, model, num_similar=5):\n",
        "    query_vectorized = vectorizer.transform([query.lower()])\n",
        "    cluster_labels = model.labels_\n",
        "\n",
        "    # Compute cosine similarity between the query and all recipes\n",
        "    similarities = cosine_similarity(query_vectorized, X)\n",
        "\n",
        "    # Find the indices of recipes with high similarity\n",
        "    similar_indices = np.where(similarities > 0.5)[1]\n",
        "\n",
        "    # Choose random recipes from the similar indices\n",
        "    similar_indices = np.random.choice(similar_indices, min(num_similar, len(similar_indices)), replace=False)\n",
        "    similar_recipes = sampled_data.iloc[similar_indices]['name'].tolist()\n",
        "\n",
        "    return similar_recipes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr7dCsgtAilL",
        "outputId": "6d676282-f760-4474-b854-89694695a770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar recipes to 'japanese dishes vegetarian' are:\n",
            "sesame dipping sauce\n"
          ]
        }
      ],
      "source": [
        "def find_similar_recipes_DB(query, vectorizer, model, sampled_data, num_similar=5):\n",
        "    query_vectorized = vectorizer.transform([query.lower()])\n",
        "\n",
        "    # Find the cluster label of the query\n",
        "    query_cluster_label = model.fit_predict(query_vectorized)[0]\n",
        "\n",
        "    # Find recipes belonging to the same cluster as the query\n",
        "    similar_indices = np.where(model.labels_ == query_cluster_label)[0]\n",
        "\n",
        "    # Choose random recipes from the similar indices\n",
        "    similar_indices = np.random.choice(similar_indices, min(num_similar, len(similar_indices)), replace=False)\n",
        "    similar_recipes = sampled_data.iloc[similar_indices]['name'].tolist()\n",
        "\n",
        "    return similar_recipes\n",
        "\n",
        "# Test the recommendation system\n",
        "query = \"japanese dishes vegetarian\"\n",
        "similar_recipes = find_similar_recipes_DB(query, vectorizer, dbscan_model, sampled_data)\n",
        "print(\"Similar recipes to '{}' are:\".format(query))\n",
        "for recipe in similar_recipes:\n",
        "    print(recipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc6IDYNiC2nI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44nbwksyDRFY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcWWOHKSDRIG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMCRof-DRc2",
        "outputId": "5ac4600f-c6bc-4162-d2a8-d1093b88075d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity scores for Word2Vec: [0.5722112555868694, 0.0, 0.3705523429644694, 0.41887949926500107, 0.519051335280776]\n",
            "Similarity scores for k-NN: [0.55439895120696, 0.49816089200264896, 0.5722112555868694, 0.43579303038741624, 0.3705523429644694]\n",
            "Similarity scores for K-Means: [0.0, 0.0, 0.0]\n",
            "Similarity scores for DBSCAN: [0.0]\n",
            "\n",
            "The best-performing model is: k-NN\n",
            "\n",
            "Best recipes recommended by the best-performing model:\n",
            "japanese cucumber salad   sunomono\n",
            "japanese sesame dipping sauce\n",
            "japanese noodle soup\n",
            "kabu    japanese turnip pickles\n",
            "vegetarian yakisoba\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "# Write a function to calculate similarity scores for each model's recommendations\n",
        "def calculate_similarity_scores(query, recommendations):\n",
        "    query_vectorized = vectorizer.transform([query.lower()])\n",
        "    similarity_scores = []\n",
        "    for recipe in recommendations:\n",
        "        recipe_vectorized = vectorizer.transform([recipe.lower()])\n",
        "        similarity_score = cosine_similarity(query_vectorized, recipe_vectorized)[0][0]\n",
        "        similarity_scores.append(similarity_score)\n",
        "    return similarity_scores\n",
        "\n",
        "# Compare models and choose the best-performing one\n",
        "def compare_models(query, sampled_data, embeddings, kmeans_model, vectorizer, dbscan_model):\n",
        "    word2vec_recipes = find_similar_recipes_cosie(sampled_data, query)\n",
        "    knn_recipes = find_similar_recipes_knn(query)\n",
        "    kmeans_recipes = find_similar_recipes_kmeans(query, kmeans_model)\n",
        "    dbscan_recipes = find_similar_recipes_DB(query, vectorizer, dbscan_model, sampled_data)\n",
        "\n",
        "    # Calculate similarity scores for each model's recommendations\n",
        "    similarity_scores = {}\n",
        "    similarity_scores['Word2Vec'] = calculate_similarity_scores(query, word2vec_recipes)\n",
        "    similarity_scores['k-NN'] = calculate_similarity_scores(query, knn_recipes)\n",
        "    similarity_scores['K-Means'] = calculate_similarity_scores(query, kmeans_recipes)\n",
        "    similarity_scores['DBSCAN'] = calculate_similarity_scores(query, dbscan_recipes)\n",
        "\n",
        "    # Print similarity scores\n",
        "    for model, scores in similarity_scores.items():\n",
        "        print(f\"Similarity scores for {model}: {scores}\")\n",
        "\n",
        "    # Choose the model with the highest average similarity score\n",
        "    avg_similarity_scores = {model: np.mean(scores) for model, scores in similarity_scores.items()}\n",
        "    best_model = max(avg_similarity_scores, key=avg_similarity_scores.get)\n",
        "    print(f\"\\nThe best-performing model is: {best_model}\")\n",
        "\n",
        "    # Return recipes recommended by the best-performing model\n",
        "    if best_model == 'Word2Vec':\n",
        "        return word2vec_recipes\n",
        "    elif best_model == 'k-NN':\n",
        "        return knn_recipes\n",
        "    elif best_model == 'K-Means':\n",
        "        return kmeans_recipes\n",
        "    elif best_model == 'DBSCAN':\n",
        "        return dbscan_recipes\n",
        "\n",
        "# Test the comparison function\n",
        "query = \"japanese dishes vegetarian\"\n",
        "with open('combined_embeddings.pkl', 'rb') as f:\n",
        "        embeddings = pickle.load(f)\n",
        "best_recipes = compare_models(query, sampled_data, embeddings, kmeans_model, vectorizer, dbscan_model)\n",
        "print(\"\\nBest recipes recommended by the best-performing model:\")\n",
        "for recipe in best_recipes:\n",
        "    print(recipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhHnwhTFDyPR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}